# MedVQA-Bench
This paper builds a comparative benchmark for Medical Visual Question Answering, implementing two models: a CNN-LSTM with attention and a CLIP-based LLaVA-Med. Trained on the SLAKE dataset, it evaluates accuracy and natural language generation metrics, and analyzes the modelsâ€™ clinical application value across diverse medical scenarios.
